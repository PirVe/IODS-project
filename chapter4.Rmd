# Week 4: Clustering and classification

```{r}
date()

# Making libraries available
if (!require("pacman")) install.packages("pacman") 
pacman::p_load(tidyverse, forcats, haven, janitor, psych, lm.beta, broom, writexl,sjstats, knitr, pwr, corrplot, tidyr, dplyr, ggplot2, GGally)
library(MASS)
library(corrplot)
library(tidyr)
library(dplyr)
library(ggplot2)
library(GGally)
```

This week we are taking a go at clustering and classification with RStudio's native Boston dataset. More information about the dataset can be found here: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html.

```{r}
# load the data from MASS package
data(Boston)

# explore the dataset
str(Boston)
dim(Boston)
summary(Boston)
```
There are 506 observations of 14 variables, which include crime rate per capita in town (crim) and average rooms per dwelling (rm) among others. 

```{r}
# summary of the data
summary(Boston)

# draw a histogram of each variable
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_histogram()
```

Mean crime rate (crim) per capita is 3,61, and as a curiosity, the minimum number of rooms per dwelling is 3,6. The age of the respondents is skewed towards very old, as there are a few who are 100 years old. Number of rooms in dwelling (rm) looks like it could be normally distributed, noting that the average number of rooms is over 6 per dwelling. Big dwellings!


```{r}
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, order = 'AOE', cl.pos = 'b', tl.pos = 'd',
         col = COL2('PRGn'), diag = FALSE)
```

In the matrix above, the bigger and darker the circle, the stronger the correlation between variables. The correlation matrix shows that there is a strong correlation between age and nox (nitrogen oxides concentration (parts per 10 million)). Also the correlation is strong between nox and indus (proportion of non-retail business acres per town), which is easy to understand, when we know that industry is a remarkable pollutor in cities. Chas is a dummy variable (Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)) and therefore it is not surprising that there are very little correlation between chas and the other variables.

To allow for easier handling of the data in the next steps, we'll scale the data by subtracting the column means from the corresponding columns and dividing the difference with standard deviation. The Boston dataset contains only numerical variables.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)
```

The scaling affects the data so that all variables are now centered around the mean value of 0. Let's take a closer look at the scaled variable "crim" which will be shown divided into quantiles, then we removed it from the scaled Boston dataset, and replaced with the categorical variable "crime":

```{r}
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Now we can divide the data into test and training sets so that 80 % of the data belongs to the training dataset.

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set  and check the names of the variables
test <- boston_scaled[-ind,]
colnames(test)
```
Next we are going to fit a linear discriminant analysis (LDA) model to the training data. we'll use the newly created  categorical variable "crime" as the target and all other variables as the predictors.
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes) + title("LDA biplot")
lda.arrows(lda.fit, myscale = 2)
```
To continue testing, we are going to remove the "crime" categorical variable from the test data set. No worries, the crime categories will be saved for later use.

```{r}
# save the crime classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

Then we'll use the LDA model to predict the classes on the test dataset. Correct classes are the saved crame categories from the test data, and we can see that 
```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```
From the cross tabulation presented above we can see that there is a significant amount of misclassification related to lower to medium levels of criminality, but the high crime rate is predicted much more accurately with only single misclassifications. This may be understandable from the assumption that high crime is generally better recognized and subsequently recorded, whereas low crime rates imply fewer observations of crime. 

Now, back to the Boston dataset, we'll calculate the Euclidean distances to look at similarity of the objects. To be able to do this, we'll scale the data first to make the observations comparable:
```{r}
# reload the dataset
data(Boston)

# standardize the data
boston_scaled2 <- scale(Boston)

# summaries of the newly scaled variables
summary(boston_scaled2)

# euclidean distance matrix
dist_eu <- dist(boston_scaled2)

# look at the summary of the distances
summary(dist_eu)

```
Running k-means algorithm on the data:
```{r}
# k-means clustering
km <-kmeans(boston_scaled2, centers = 3)
```

Now investigating what is the optimal number of clusters: 
```{r}
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled2)

# determine the k
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```
There is a big drop in similarity between the clusters at about 2, so the optimal number of clusters is 2. Running the k-means clustering algorithm with 2 clusters on the boston_scaled2 data:

```{r}
# run k-means clustering algorithm again 
km <-kmeans(boston_scaled2, centers = 2)

# plot the boston_scaled2 dataset with clusters
par(mfrow = c(2,2))
pairs(boston_scaled2[1:5], col = km$cluster)
pairs(boston_scaled2[6:10], col = km$cluster)
pairs(boston_scaled2, col = km$cluster)
```
The dummy variable chas is distinguished by the almost complete lack of overlap between the clusters. Age and pupil-teacher ratio per town (ptratio). Crime rate variation correlates with human variables such as age, black, lstat (lower status of population), which is not surprising.  
No bonus tasks done this time.